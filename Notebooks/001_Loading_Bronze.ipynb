{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b2388b6-36ce-4cc9-b971-03d9e96a12e2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Configurações iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30925cf6-782d-4983-b519-2eb372715515",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importar as bibliotecas necessárias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Iniciar a SparkSession com configurações otimizadas\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Load Data Bronze\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")  \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\") \\\n",
    "    .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Definir caminhos de armazenamento no Data Lake\n",
    "lz_path_in = \"/FileStore/lhdw/landingzone/vendas/processar\"\n",
    "lz_path_out = \"/FileStore/lhdw/landingzone/vendas/processado\"\n",
    "bronze_path = \"/FileStore/lhdw/bronze/vendas\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53103768-8708-4a5c-b7fd-50f51fd7dcd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbutils.fs.cp('/FileStore/lhdw/landingzone/vendas/processado/dados_2012.csv',lz_path_in)\n",
    "#dbutils.fs.rm('/FileStore/lhdw/landingzone/vendas/processado/dados_2012.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40b9021f-a833-409f-8855-f6397b2801bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Criando schema dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73a5f0fe-a5f7-48bb-a255-b3403b07bec2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema_lz = StructType([\n",
    "    StructField(\"IDProduto\", IntegerType(), True),\n",
    "    StructField(\"Data\", DateType(), True),\n",
    "    StructField(\"IDCliente\", IntegerType(), True),\n",
    "    StructField(\"IDCampanha\", IntegerType(), True),\n",
    "    StructField(\"Unidades\", IntegerType(), True),\n",
    "    StructField(\"Produto\", StringType(), True),\n",
    "    StructField(\"Categoria\", StringType(), True),\n",
    "    StructField(\"Segmento\", StringType(), True),\n",
    "    StructField(\"IDFabricante\", IntegerType(), True),\n",
    "    StructField(\"Fabricante\", StringType(), True),\n",
    "    StructField(\"CustoUnitario\", DoubleType(), True),\n",
    "    StructField(\"PrecoUnitario\", DoubleType(), True),\n",
    "    StructField(\"CodigoPostal\", StringType(), True),\n",
    "    StructField(\"EmailNome\", StringType(), True),\n",
    "    StructField(\"Cidade\", StringType(), True),\n",
    "    StructField(\"Estado\", StringType(), True),\n",
    "    StructField(\"Regiao\", StringType(), True),\n",
    "    StructField(\"Distrito\", StringType(), True),\n",
    "    StructField(\"Pais\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd3a604d-cb1e-4469-8edb-48ee7e7cbdd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Importando arquivo csv e Salvando na camada BRONZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bc765e1-ff35-4889-ac39-50320ce752d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lista o conteúdo da pasta\n",
    "arquivos = dbutils.fs.ls(lz_path_in)\n",
    "\n",
    "for arquivo in arquivos: \n",
    "    #Importando o csv e adicionando uma coluna com o nome do arquivo importado.\n",
    "    df_vendas = spark.read.option(\"header\",\"true\").schema(schema_lz).csv(f'{lz_path_in}/{arquivo.name}') \\\n",
    "        .withColumn(\"filename\", regexp_extract(input_file_name(), \"([^/]+)$\", 0))\n",
    "\n",
    "    #Adicionar colunas de Ano e Mes\n",
    "    df_vendas = df_vendas.withColumn(\"Ano\",year(\"Data\"))\\\n",
    "        .withColumn(\"Mes\",month(\"Data\"))\n",
    "\n",
    "    #Separando nome dos arquivos processados para serem movidos para o diretorio processado.\n",
    "    file_name = df_vendas.select('filename').distinct()\n",
    "    \n",
    "    #Exportar arquivo em formato Parquet particionado por Ano e Mes\n",
    "    df_vendas.write.mode(\"overwrite\").partitionBy(\"Ano\",\"Mes\").parquet(f'{bronze_path}/{arquivo.name}')\n",
    "\n",
    "    #Cria um CSV com os arquivos que foram processados para ser utilizados na camada Silver.\n",
    "    file_name.write.mode(\"append\").csv(f'{bronze_path}/files_to_silver_process',header=True)\n",
    "\n",
    "    #Move o arquivo processado para o diretório de Processados.\n",
    "    dbutils.fs.cp(f'{lz_path_in}/{arquivo.name}', lz_path_out)\n",
    "    dbutils.fs.rm(f'{lz_path_in}/{arquivo.name}')\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8649106c-52ba-4268-9612-6aa99feafacf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Deletando caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b221d3c-5358-45ac-a3ce-c40beb9e8d37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1903414303381730,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "001_Loading_Bronze",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}