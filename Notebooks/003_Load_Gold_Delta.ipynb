{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaf6e41b-7ff9-4e74-bee9-773971748234",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Camada Gold Delta - Criação das dimensões e fatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d7c820a-e3ce-4723-8404-9646bc908a1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession with the required configurations for Delta Lake\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Carga Delta\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "019343a0-401a-4b85-8f8a-bbe8559cc34f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Definindo variáveis com os caminhos de armazenamento\n",
    "silver_path = '/FileStore/lhdw/silver/vendas'\n",
    "gold_path = '/FileStore/lhdw/gold/vendas'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "730ee255-ee8e-42e8-8bae-8ebd306515f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Importando dados da camada silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "861ab63d-fc34-40cc-891b-d7548654cad9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver = spark.read.parquet(silver_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d2b0b8d-0777-47ae-ac5d-270ac3716409",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Criando Dimensão DIM_PRODUTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a71b1e45-4cbb-4325-afda-585e114c043a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "#Nome tabela destino\n",
    "tbl_destino = \"dim_produto\"\n",
    "\n",
    "#Selecionando as colunas e excluindo os registros duplicados\n",
    "df_dim_produto = df_silver.select('IDProduto','Produto','Categoria')\\\n",
    "                          .dropDuplicates()\\\n",
    "                          .orderBy('IDProduto')\n",
    "\n",
    "#Adicionando a chave primária\n",
    "df_dim_produto = df_dim_produto.withColumn('sk_produto', monotonically_increasing_id()+1)\n",
    "\n",
    "#Criando a tabela gold DIM_PRODUTO no formato Delta\n",
    "df_dim_produto.write.format('delta').mode('overwrite').save(f\"{gold_path}/{tbl_destino}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a43875aa-c5e5-4cdd-8b28-bfb9fee6b2d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Criando dimensão DIM CATEGORIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9863b0d2-904d-46e9-ba70-add29b6db755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "#Nome tabela destino\n",
    "tbl_destino = \"dim_categoria\"\n",
    "\n",
    "#Selecionando as colunas e excluindo os registros duplicados\n",
    "df_dim_categoria = df_silver.select('Categoria')\\\n",
    "                          .distinct()\\\n",
    "                          .orderBy('Categoria')\n",
    "\n",
    "#Adicionando a chave primária\n",
    "df_dim_categoria = df_dim_categoria.withColumn('sk_categoria', monotonically_increasing_id()+1)\n",
    "\n",
    "#Criando a tabela gold DIM_CATEGORIA no formato Delta\n",
    "df_dim_categoria.write.format('delta').mode('overwrite').save(f\"{gold_path}/{tbl_destino}\")\n",
    "\n",
    "#display(df_dim_categoria)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf310ebb-a0c7-4b77-bccb-8daa09a3025a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Criando dimensão DIM SEGMENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1fafacb-947f-42f9-8bf6-d96be3f162ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "#Nome tabela destino\n",
    "tbl_destino = \"dim_segmento\"\n",
    "\n",
    "#Selecionando as colunas e excluindo os registros duplicados\n",
    "df_dim_segmento = df_silver.select('Segmento')\\\n",
    "                          .distinct()\\\n",
    "                          .orderBy('Segmento')\n",
    "\n",
    "#Adicionando a chave primária\n",
    "df_dim_segmento = df_dim_segmento.withColumn('sk_segmento', monotonically_increasing_id()+1)\n",
    "\n",
    "#Criando a tabela gold DIM_SEGMENTO no formato Delta\n",
    "df_dim_segmento.write.format('delta').mode('overwrite').save(f\"{gold_path}/{tbl_destino}\")\n",
    "\n",
    "#display(df_dim_segmento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bfb8a86-e541-4c09-93c4-3e9e0a4285d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Criando tabela dimensão DIM FABRICANTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd3b432c-9c6a-41be-b68c-237cff286409",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "#Nome tabela destino\n",
    "tbl_destino = \"dim_fabricante\"\n",
    "\n",
    "#Selecionando as colunas e excluindo os registros duplicados\n",
    "df_dim_fabricante = df_silver.select('IDFabricante','Fabricante')\\\n",
    "                          .distinct()\\\n",
    "                          .orderBy('IDFabricante')\n",
    "\n",
    "#Adicionando a chave primária\n",
    "df_dim_fabricante = df_dim_fabricante.withColumn('sk_fabricante', monotonically_increasing_id()+1)\n",
    "\n",
    "#Criando a tabela gold DIM_FABRICANTE no formato Delta\n",
    "df_dim_fabricante.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").save(f\"{gold_path}/{tbl_destino}\")\n",
    "\n",
    "#display(df_dim_fabricante)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b4c7949-0908-4496-86ae-78ad80ddddf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Criando dimensão DIM GEOGRAFIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82a104db-b3ec-4a6a-b253-f124fdaed6ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "#Nome tabela destino\n",
    "tbl_destino = \"dim_geografia\"\n",
    "\n",
    "#Selecionando as colunas e excluindo os registros duplicados\n",
    "df_dim_geografia = df_silver.select('Cidade','Estado','Regiao','Distrito','Pais','CodigoPostal')\\\n",
    "                          .dropDuplicates()\n",
    "\n",
    "#Adicionando a chave primária\n",
    "df_dim_geografia = df_dim_geografia.withColumn('sk_geografia', monotonically_increasing_id()+1)\n",
    "\n",
    "#Criando a tabela gold DIM_GEOGRAFIA no formato Delta\n",
    "df_dim_geografia.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").save(f\"{gold_path}/{tbl_destino}\")\n",
    "\n",
    "#display(df_dim_geografia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "388e7c25-ce91-4b92-bc54-710560bcece3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Criando dimensão DIM CLIENTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc95478a-322a-4f02-8b49-c1c6c56efdef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, monotonically_increasing_id\n",
    "\n",
    "#Nome tabela destino\n",
    "tbl_destino = \"dim_cliente\"\n",
    "\n",
    "#1-Extraindo clientes únicos para criar a dimensão clientes\n",
    "df_dim_cliente = df_silver.select('IDCliente','Nome','Email','Cidade','Estado','Regiao','Distrito','Pais','CodigoPostal')\\\n",
    "                          .distinct()\n",
    "\n",
    "#2-Fazendo join para buscar a SK_GEOGRAFIA da DIM_GEOGRAFIA\n",
    "df_dim_cliente_completo = df_dim_cliente.alias('cli')\\\n",
    "        .join(df_dim_geografia.alias('geo'),\n",
    "              (col('cli.Cidade')==col('geo.Cidade')) &\n",
    "              (col('cli.Estado')==col('geo.Estado')) &\n",
    "              (col('cli.Regiao')==col('geo.Regiao')) &\n",
    "              (col('cli.Distrito') == col('geo.Distrito')) &\n",
    "              (col('cli.Pais') == col('geo.Pais')) &\n",
    "              (col('cli.CodigoPostal') == col('geo.CodigoPostal')),\n",
    "              'left')\\\n",
    "        .select('cli.IDCliente','cli.Nome','cli.Email','geo.sk_geografia').orderBy('IDCliente')\n",
    "\n",
    "#3-Criando a chave única de Clientes.\n",
    "df_dim_cliente_completo = df_dim_cliente_completo.withColumn('sk_cliente',monotonically_increasing_id()+1)\n",
    "\n",
    "#4-#Criando a tabela gold DIM_CLIENTE no formato Delta\n",
    "df_dim_cliente_completo.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").save(f\"{gold_path}/{tbl_destino}\")\n",
    "\n",
    "#display(df_dim_cliente_completo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da88f097-c0cb-4ce3-9352-63e0a7ab2b59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Criando da tabela fato FT_VENDAS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3873b172-feca-4c92-9c10-0d5133b59bfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast,year, month\n",
    "\n",
    "#Nome tabela destino\n",
    "tbl_destino = 'ft_vendas'\n",
    "\n",
    "# Juntar dados da Silver com tabelas de dimensões para obter as chaves substitutas\n",
    "df_ft_vendas = df_silver.alias('s')\\\n",
    "    .join(broadcast(df_dim_produto.select('IDProduto','sk_produto')),'IDProduto')\\\n",
    "    .join(broadcast(df_dim_categoria.select('Categoria','sk_categoria')),'Categoria')\\\n",
    "    .join(broadcast(df_dim_segmento.select('Segmento','sk_segmento')),'Segmento')\\\n",
    "    .join(broadcast(df_dim_fabricante.select('IDFabricante','sk_fabricante')),'IDFabricante')\\\n",
    "    .join(broadcast(df_dim_cliente_completo.select('IDCliente','sk_cliente')),'IDCliente')\\\n",
    "    .select(col('s.Data').alias('DataVenda'),\n",
    "            'sk_produto',\n",
    "            'sk_categoria',\n",
    "            'sk_segmento',\n",
    "            'sk_fabricante',\n",
    "            'sk_cliente',\n",
    "            'Unidades',\n",
    "            col('s.PrecoUnitario'),\n",
    "            col('s.CustoUnitario'),\n",
    "            col('s.TotalVendas'))\n",
    "\n",
    "df_ft_vendas.withColumn('Ano',year('DataVenda'))\\\n",
    "            .withColumn('Mes',month('DataVenda'))\\\n",
    "            .write.format('Delta')\\\n",
    "            .option('MaxRecordsPerFile', 1000000)\\\n",
    "            .partitionBy('Ano','Mes')\\\n",
    "            .save(f'{gold_path}/{tbl_destino}')\n",
    "\n",
    "#display(df_ft_vendas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b6ed686-941d-4f1c-965c-06acc37bbe41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Fazendo Limpeza da Memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f65889ad-12f5-4d39-834a-101c21badbf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Coletar lixo para liberar memória\n",
    "gc.collect()\n",
    "\n",
    "# Limpar todos os dados em cache\n",
    "spark.catalog.clearCache()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2429770638915619,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "003_Load_Gold_Delta",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}