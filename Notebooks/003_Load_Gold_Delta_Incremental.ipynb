{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaf6e41b-7ff9-4e74-bee9-773971748234",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Camada Gold Delta - Criação das dimensões e fatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d7c820a-e3ce-4723-8404-9646bc908a1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Create a SparkSession with the required configurations for Delta Lake\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Carga Delta\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "019343a0-401a-4b85-8f8a-bbe8559cc34f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Definindo variáveis com os caminhos de armazenamento\n",
    "silver_path = '/FileStore/lhdw/silver/vendas'\n",
    "gold_path = '/FileStore/lhdw/gold/vendas'\n",
    "gold_fato_path = '/FileStore/lhdw/gold/vendas/ft_vendas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0a50798-d61b-4135-ae3f-6a53f4e336dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Verificar a ultima data de venda carregada.\n",
    "from pyspark.sql.functions import max\n",
    "max_data_venda = spark.read.format(\"delta\").load(gold_fato_path)\\\n",
    "    .select(max('DataVenda')).collect()[0][0]\n",
    "print(max_data_venda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "730ee255-ee8e-42e8-8bae-8ebd306515f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Importando dados da camada silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "861ab63d-fc34-40cc-891b-d7548654cad9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(spark.read.parquet(silver_path))\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "#Carregando dados da Silver maiores que a ultima data da Gold\n",
    "df_silver = spark.read.parquet(silver_path)\\\n",
    "        .filter(f'Data > \"{max_data_venda}\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d2b0b8d-0777-47ae-ac5d-270ac3716409",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Criando Dimensão DIM_PRODUTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a71b1e45-4cbb-4325-afda-585e114c043a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "#Nome tabela destino\n",
    "tbl_destino = \"dim_produto\"\n",
    "\n",
    "#Nome da coluna sk\n",
    "sk = 'sk_produto'\n",
    "\n",
    "#Lendo Dim Produto para filtrar somente o que ainda não foi carregado.\n",
    "df_dim_produto = spark.read.format('delta').load(f'{gold_path}/{tbl_destino}')\\\n",
    "                .select('IDProduto')\n",
    "\n",
    "#Lendo Dim Produto para pegar a proxima SK.\n",
    "max_sk_produto = spark.read.format(\"delta\").load(f'{gold_path}/{tbl_destino}')\\\n",
    "    .select(max(sk)+1).collect()[0][0]\n",
    "\n",
    "#Selecionando as colunas e excluindo os registros duplicados\n",
    "df_dim_produto_new = df_silver.select('IDProduto','Produto','Categoria')\\\n",
    "                          .dropDuplicates()\\\n",
    "                          .orderBy('IDProduto')\n",
    "\n",
    "#Filtrando somente os novos produtos\n",
    "df_dim_produto_new = df_dim_produto_new.join(df_dim_produto,df_dim_produto_new.IDProduto == df_dim_produto.IDProduto,'anti')\n",
    "\n",
    "#Adicionando a chave primária\n",
    "df_dim_produto_new = df_dim_produto_new.withColumn('sk_produto', monotonically_increasing_id()+max_sk_produto)\n",
    "\n",
    "#Criando a tabela gold DIM_PRODUTO no formato Delta\n",
    "df_dim_produto_new.write.format('delta').mode('append').save(f\"{gold_path}/{tbl_destino}\")\n",
    "\n",
    "#Carregando Dim Produto completa para carregar a FATO.\n",
    "df_dim_produto = spark.read.format('delta').load(f'{gold_path}/{tbl_destino}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a43875aa-c5e5-4cdd-8b28-bfb9fee6b2d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Criando dimensão DIM CATEGORIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9863b0d2-904d-46e9-ba70-add29b6db755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "#Nome tabela destino\n",
    "tbl_destino = \"dim_categoria\"\n",
    "\n",
    "#Nome da coluna sk\n",
    "sk = 'sk_categoria'\n",
    "\n",
    "#Lendo Dim Categoria para filtrar somente o que ainda não foi carregado.\n",
    "df_dim_categoria = spark.read.format('delta').load(f'{gold_path}/{tbl_destino}')\\\n",
    "                .select('Categoria')\n",
    "\n",
    "#Lendo Dim Categoria para pegar a proxima SK.\n",
    "max_sk_categoria = spark.read.format(\"delta\").load(f'{gold_path}/{tbl_destino}')\\\n",
    "    .select(max(sk)+1).collect()[0][0]\n",
    "\n",
    "#Selecionando as colunas e excluindo os registros duplicados\n",
    "df_dim_categoria_new = df_silver.select('Categoria')\\\n",
    "                          .distinct()\\\n",
    "                          .orderBy('Categoria')\n",
    "\n",
    "#Filtrando somente as novas categorias\n",
    "df_dim_categoria_new = df_dim_categoria_new.join(df_dim_categoria,df_dim_categoria_new.Categoria == df_dim_categoria.Categoria,'anti')                          \n",
    "\n",
    "#Adicionando a chave primária\n",
    "df_dim_categoria_new = df_dim_categoria_new.withColumn('sk_categoria', monotonically_increasing_id()+max_sk_categoria)\n",
    "\n",
    "#Criando a tabela gold DIM_CATEGORIA no formato Delta\n",
    "df_dim_categoria_new.write.format('delta').mode('append').save(f\"{gold_path}/{tbl_destino}\")\n",
    "\n",
    "#display(df_dim_categoria_new)\n",
    "\n",
    "#Carregando DIM_CATEGORIA completa para carregar a FATO.\n",
    "df_dim_categoria = spark.read.format('delta').load(f'{gold_path}/{tbl_destino}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf310ebb-a0c7-4b77-bccb-8daa09a3025a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Criando dimensão DIM SEGMENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1fafacb-947f-42f9-8bf6-d96be3f162ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "#Nome tabela destino\n",
    "tbl_destino = \"dim_segmento\"\n",
    "\n",
    "#Nome da coluna sk\n",
    "sk = 'sk_segmento'\n",
    "\n",
    "#Lendo Dim Segmento para filtrar somente o que ainda não foi carregado.\n",
    "df_dim_segmento = spark.read.format('delta').load(f'{gold_path}/{tbl_destino}')\\\n",
    "                .select('Segmento')\n",
    "\n",
    "#Lendo Dim Segmento para pegar a proxima SK.\n",
    "max_sk_segmento = spark.read.format(\"delta\").load(f'{gold_path}/{tbl_destino}')\\\n",
    "    .select(max(sk)+1).collect()[0][0]\n",
    "\n",
    "#Selecionando as colunas e excluindo os registros duplicados\n",
    "df_dim_segmento_new = df_silver.select('Segmento')\\\n",
    "                          .distinct()\\\n",
    "                          .orderBy('Segmento')\n",
    "\n",
    "#Filtrando somente os novos segmentos\n",
    "df_dim_segmento_new = df_dim_segmento_new.join(df_dim_segmento,df_dim_segmento_new.Segmento == df_dim_segmento.Segmento,'anti')                          \n",
    "\n",
    "#Adicionando a chave primária\n",
    "df_dim_segmento_new = df_dim_segmento_new.withColumn('sk_segmento', monotonically_increasing_id()+max_sk_segmento)\n",
    "\n",
    "#Criando a tabela gold DIM_SEGMENTO no formato Delta\n",
    "df_dim_segmento_new.write.format('delta').mode('append').save(f\"{gold_path}/{tbl_destino}\")\n",
    "\n",
    "#display(df_dim_segmento_new)\n",
    "\n",
    "#Carregando DIM_SEGMENTO completa para carregar a FATO.\n",
    "df_dim_segmento = spark.read.format('delta').load(f'{gold_path}/{tbl_destino}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bfb8a86-e541-4c09-93c4-3e9e0a4285d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Criando tabela dimensão DIM FABRICANTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd3b432c-9c6a-41be-b68c-237cff286409",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "#Nome tabela destino\n",
    "tbl_destino = \"dim_fabricante\"\n",
    "\n",
    "#Nome da coluna sk\n",
    "sk = 'sk_fabricante'\n",
    "\n",
    "#Lendo Dim Fabricante para filtrar somente o que ainda não foi carregado.\n",
    "df_dim_fabricante = spark.read.format('delta').load(f'{gold_path}/{tbl_destino}')\\\n",
    "                .select('IDFabricante')\n",
    "\n",
    "#Lendo Dim Fabricante para pegar a proxima SK.\n",
    "max_sk_fabricante = spark.read.format(\"delta\").load(f'{gold_path}/{tbl_destino}')\\\n",
    "    .select(max(sk)+1).collect()[0][0]\n",
    "\n",
    "#Selecionando as colunas e excluindo os registros duplicados\n",
    "df_dim_fabricante_new = df_silver.select('IDFabricante','Fabricante')\\\n",
    "                          .dropDuplicates()\\\n",
    "                          .orderBy('IDFabricante')\n",
    "\n",
    "#Filtrando somente os novos fabricantes\n",
    "df_dim_fabricante_new = df_dim_fabricante_new.join(df_dim_fabricante,df_dim_fabricante_new.IDFabricante == df_dim_fabricante.IDFabricante,'anti')\n",
    "\n",
    "#Adicionando a chave primária\n",
    "df_dim_fabricante_new = df_dim_fabricante_new.withColumn('sk_fabricante', monotonically_increasing_id()+max_sk_fabricante)\n",
    "\n",
    "#Criando a tabela gold DIM_FABRICANTE no formato Delta\n",
    "df_dim_fabricante_new.write.format('delta').mode('append').save(f\"{gold_path}/{tbl_destino}\")\n",
    "\n",
    "#display(df_dim_fabricante_new)\n",
    "\n",
    "#Carregando DIM_FABRICANTE completa para carregar a FATO.\n",
    "df_dim_fabricante = spark.read.format('delta').load(f'{gold_path}/{tbl_destino}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b4c7949-0908-4496-86ae-78ad80ddddf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Criando dimensão DIM GEOGRAFIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82a104db-b3ec-4a6a-b253-f124fdaed6ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "#Nome tabela destino\n",
    "tbl_destino = \"dim_geografia\"\n",
    "\n",
    "#Nome da coluna sk\n",
    "sk = 'sk_geografia'\n",
    "\n",
    "#Lendo Dim Geografia para filtrar somente o que ainda não foi carregado.\n",
    "df_dim_geografia = spark.read.format('delta').load(f'{gold_path}/{tbl_destino}')\n",
    "\n",
    "#Lendo Dim Geografia para pegar a proxima SK.\n",
    "max_sk_geografia = spark.read.format(\"delta\").load(f'{gold_path}/{tbl_destino}')\\\n",
    "    .select(max(sk)+1).collect()[0][0]\n",
    "\n",
    "#Selecionando as colunas e excluindo os registros duplicados\n",
    "df_dim_geografia_new = df_silver.select('Cidade','Estado','Regiao','Distrito','Pais','CodigoPostal')\\\n",
    "                          .dropDuplicates()\n",
    "\n",
    "#Filtrando somente as novas Geografias\n",
    "df_dim_geografia_new = df_dim_geografia_new.join(df_dim_geografia,\\\n",
    "          (df_dim_geografia_new.Cidade == df_dim_geografia.Cidade) &\n",
    "          (df_dim_geografia_new.Estado == df_dim_geografia.Estado) &\n",
    "          (df_dim_geografia_new.Regiao == df_dim_geografia.Regiao) &\n",
    "          (df_dim_geografia_new.Distrito == df_dim_geografia.Distrito) &\n",
    "          (df_dim_geografia_new.Pais == df_dim_geografia.Pais) &\n",
    "          (df_dim_geografia_new.CodigoPostal == df_dim_geografia.CodigoPostal), \n",
    "          'anti')\n",
    "\n",
    "#Adicionando a chave primária\n",
    "df_dim_geografia_new = df_dim_geografia_new.withColumn('sk_geografia', monotonically_increasing_id()+max_sk_geografia)\n",
    "\n",
    "#Criando a tabela gold DIM_GEOGRAFIA no formato Delta\n",
    "df_dim_geografia_new.write.format('delta').mode('append').option(\"overwriteSchema\", \"true\").save(f\"{gold_path}/{tbl_destino}\")\n",
    "\n",
    "#display(df_dim_geografia_new)\n",
    "\n",
    "#Carregando DIM_GEOGRAFIA completa para carregar a FATO.\n",
    "df_dim_geografia = spark.read.format('delta').load(f'{gold_path}/{tbl_destino}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "388e7c25-ce91-4b92-bc54-710560bcece3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Criando dimensão DIM CLIENTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc95478a-322a-4f02-8b49-c1c6c56efdef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, monotonically_increasing_id\n",
    "\n",
    "#Nome tabela destino\n",
    "tbl_destino = \"dim_cliente\"\n",
    "\n",
    "#Nome da coluna sk\n",
    "sk = 'sk_cliente'\n",
    "\n",
    "#Lendo Gold Dim Cliente para filtrar somente o que ainda não foi carregado.\n",
    "df_dim_cliente = spark.read.format('delta').load(f'{gold_path}/{tbl_destino}')\\\n",
    "                  .select('IDCliente')\n",
    "\n",
    "#Lendo Dim Cliente para pegar a proxima SK.\n",
    "max_sk_cliente = spark.read.format(\"delta\").load(f'{gold_path}/{tbl_destino}')\\\n",
    "    .select(max(sk)+1).collect()[0][0]\n",
    "\n",
    "#1-Extraindo clientes únicos para criar a dimensão clientes\n",
    "df_dim_cliente_new = df_silver.select('IDCliente','Nome','Email','Cidade','Estado','Regiao','Distrito','Pais','CodigoPostal')\\\n",
    "                          .distinct()\n",
    "\n",
    "#Filtrando somente os novos Clientes\n",
    "df_dim_cliente_new = df_dim_cliente_new.join(df_dim_cliente,df_dim_cliente_new.IDCliente == df_dim_cliente.IDCliente,'anti')\n",
    "\n",
    "#2-Fazendo join para buscar a SK_GEOGRAFIA da DIM_GEOGRAFIA\n",
    "df_dim_cliente_completo = df_dim_cliente_new.alias('cli')\\\n",
    "        .join(df_dim_geografia.alias('geo'),\n",
    "              (col('cli.Cidade')==col('geo.Cidade')) &\n",
    "              (col('cli.Estado')==col('geo.Estado')) &\n",
    "              (col('cli.Regiao')==col('geo.Regiao')) &\n",
    "              (col('cli.Distrito') == col('geo.Distrito')) &\n",
    "              (col('cli.Pais') == col('geo.Pais')) &\n",
    "              (col('cli.CodigoPostal') == col('geo.CodigoPostal')),\n",
    "              'left')\\\n",
    "        .select('cli.IDCliente','cli.Nome','cli.Email','geo.sk_geografia').orderBy('IDCliente')\n",
    "\n",
    "#3-Criando a chave única de Clientes.\n",
    "df_dim_cliente_completo = df_dim_cliente_completo.withColumn('sk_cliente',monotonically_increasing_id()+max_sk_cliente)\n",
    "\n",
    "#4-#Criando a tabela gold DIM_CLIENTE no formato Delta\n",
    "df_dim_cliente_completo.write.format('delta').mode('append').option(\"overwriteSchema\", \"true\").save(f\"{gold_path}/{tbl_destino}\")\n",
    "\n",
    "#display(df_dim_cliente_completo)\n",
    "\n",
    "#Carregando DIM_CLIENTE completa para carregar a FATO.\n",
    "df_dim_cliente = spark.read.format('delta').load(f'{gold_path}/{tbl_destino}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5295f5a-4979-430e-aead-6ff45e58b898",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(spark.read.format('Delta').load(f'{gold_path}/dim_cliente').count()) \n",
    "#display(spark.read.parquet(f'{silver_path}').select('IDCliente').distinct().count())\n",
    "\n",
    "#223734\n",
    "#223734"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da88f097-c0cb-4ce3-9352-63e0a7ab2b59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Criando da tabela fato FT_VENDAS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3873b172-feca-4c92-9c10-0d5133b59bfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast,year, month\n",
    "\n",
    "#Nome tabela destino\n",
    "tbl_destino = 'ft_vendas'\n",
    "\n",
    "# Juntar dados da Silver com tabelas de dimensões para obter as chaves substitutas\n",
    "df_ft_vendas = df_silver.alias('s')\\\n",
    "    .join(broadcast(df_dim_produto.select('IDProduto','sk_produto')),'IDProduto')\\\n",
    "    .join(broadcast(df_dim_categoria.select('Categoria','sk_categoria')),'Categoria')\\\n",
    "    .join(broadcast(df_dim_segmento.select('Segmento','sk_segmento')),'Segmento')\\\n",
    "    .join(broadcast(df_dim_fabricante.select('IDFabricante','sk_fabricante')),'IDFabricante')\\\n",
    "    .join(broadcast(df_dim_cliente.select('IDCliente','sk_cliente')),'IDCliente')\\\n",
    "    .select(col('s.Data').alias('DataVenda'),\n",
    "            'sk_produto',\n",
    "            'sk_categoria',\n",
    "            'sk_segmento',\n",
    "            'sk_fabricante',\n",
    "            'sk_cliente',\n",
    "            'Unidades',\n",
    "            col('s.PrecoUnitario'),\n",
    "            col('s.CustoUnitario'),\n",
    "            col('s.TotalVendas'))\n",
    "\n",
    "df_ft_vendas.withColumn('Ano',year('DataVenda'))\\\n",
    "            .withColumn('Mes',month('DataVenda'))\\\n",
    "            .write.format('Delta')\\\n",
    "            .mode(\"append\")\\\n",
    "            .option('MaxRecordsPerFile', 1000000)\\\n",
    "            .partitionBy('Ano','Mes')\\\n",
    "            .save(f'{gold_path}/{tbl_destino}')\n",
    "\n",
    "#display(df_ft_vendas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a3f7777-2810-4b36-b1fa-111d32bc2ea8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbutils.fs.rm('/FileStore/lhdw/gold/vendas/FT_VENDAS',recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52026337-4164-4555-af75-3964ab7443cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "/* Validando a tabela delta criada*/\n",
    "/* \n",
    "select * from delta.`/FileStore/lhdw/gold/vendas/dim_produto` \n",
    "select * from delta.`/FileStore/lhdw/gold/vendas/dim_categoria` \n",
    "select * from delta.`/FileStore/lhdw/gold/vendas/dim_segmento` \n",
    "select * from delta.`/FileStore/lhdw/gold/vendas/dim_fabricante` \n",
    "select * from delta.`/FileStore/lhdw/gold/vendas/dim_geografia`\n",
    "select * from delta.`/FileStore/lhdw/gold/vendas/dim_cliente`\n",
    "select * from delta.`/FileStore/lhdw/gold/vendas/ft_vendas`\n",
    "select ano, sum(TotalVendas), count(*) from delta.`/FileStore/lhdw/gold/vendas/ft_vendas`\n",
    "group by ano\n",
    "*/\n",
    "\n",
    "/*2013\t12244978.910005424\t124777\n",
    "2012\t11395367.170004882\t116857\n",
    "2011\t10595198.130001815\t112202*/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7e667d4-6a82-43d5-a8e9-172a56142b39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Validando Silver\n",
    "df_silver = spark.read.parquet('/FileStore/lhdw/silver/vendas')\n",
    "df_silver = df_silver.withColumn('TotalVendas',df_silver.TotalVendas.cast('double'))\n",
    "#display(df_silver)\n",
    "display(df_silver.groupBy('Ano').agg(F.sum('TotalVendas').alias('vendas')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b6ed686-941d-4f1c-965c-06acc37bbe41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Fazendo Limpeza da Memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f65889ad-12f5-4d39-834a-101c21badbf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Coletar lixo para liberar memória\n",
    "gc.collect()\n",
    "\n",
    "# Limpar todos os dados em cache\n",
    "spark.catalog.clearCache()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 846327649134028,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "003_Load_Gold_Delta_Incremental",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}